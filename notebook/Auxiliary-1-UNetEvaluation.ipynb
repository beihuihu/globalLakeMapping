{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Author: Ankit Kariryaa, University of Bremen\n",
    "  \n",
    "  Modified by Xuehui Pi, Qiuqi Luo and Beihui Hu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np               # numerical array manipulation\n",
    "import pandas as pd\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import time\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "import rasterio                  # I/O raster data (netcdf, height, geotiff, ...)\n",
    "import rasterio.warp             # Reproject raster samples\n",
    "import rasterio.mask\n",
    "from core.losses import accuracy, dice_loss, IoU, recall,F1_score, precision\n",
    "from core.optimizers import adaDelta\n",
    "from core.frame_info import FrameInfo\n",
    "from core.dataset_generator import DataGenerator\n",
    "from core.visualize import display_images\n",
    "\n",
    "from tensorflow.keras import mixed_precision \n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "import warnings                  # ignore annoying warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto(\n",
    "    #device_count={\"CPU\": 64},\n",
    "    allow_soft_placement=True, \n",
    "    log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the data related variables used in the notebook \n",
    "\n",
    "# For reading the GSW and annotated images generated in the step - 1\n",
    "\n",
    "base_dir = r''\n",
    "dataset_dir=os.path.join(base_dir,'dataset')\n",
    "type_num = 5\n",
    "image_type = '.tif'\n",
    "ann_type = '.png'\n",
    "annotation_fn = 'annotation_type'\n",
    "image_fn = 'image_type'\n",
    "# For testing, images are divided into sequential patches \n",
    "patch_generation_stratergy = 'sequential'\n",
    "patch_size = (576,576,6) ## Height * Width * (Input or Output) channelsï¼š[GSW, ANNOTATION]\n",
    "BATCH_SIZE = 16 # Model is evaluated in batches; See https://keras.io/models/model/\n",
    "\n",
    "# # When stratergy == sequential\n",
    "step_size = (576,576)\n",
    "\n",
    "input_shape = (576,576,5)\n",
    "input_image_channel = [0,1,2,3,4]\n",
    "input_label_channel = [5]\n",
    "\n",
    "OPTIMIZER_NAME = 'adaDelta'\n",
    "OPTIMIZER = adaDelta \n",
    "# OPTIMIZER=tf.train.experimental.enable_mixed_precision_graph_rewrite(OPTIMIZER)\n",
    "OPTIMIZER =  mixed_precision.LossScaleOptimizer(OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = dice_loss\n",
    "LOSS_NAME = 'dice_loss'\n",
    "modelToEvaluate =os.path.join(base_dir,r'saved_models\\lakes_20240130-2243_AdaDelta_dice_loss_012345_576.h5')\n",
    "print(modelToEvaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File path for final report \n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "chf = input_image_channel + input_label_channel\n",
    "chs = reduce(lambda a,b: a+str(b),   chf, '')\n",
    "evaluation_report_path = model_path =  os.path.join(base_dir, 'evaluationreport') \n",
    "if not os.path.exists(evaluation_report_path):\n",
    "    os.makedirs(evaluation_report_path)\n",
    "evaluation_report_filename = os.path.join(evaluation_report_path,'evaluation_per_pixel{}_{}.csv'.format(timestr,chs))\n",
    "print(evaluation_report_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImgs(path_to_write, fn):\n",
    "    image = rasterio.open(os.path.join(path_to_write, fn))\n",
    "    read_image = image.read()\n",
    "    comb_img = np.transpose(read_image, axes=(1,2,0))\n",
    "    annotation_im = Image.open(os.path.join(path_to_write, fn.replace(image_fn,annotation_fn).replace(image_type,ann_type)))\n",
    "    annotation = np.array(annotation_im)\n",
    "    f = FrameInfo(comb_img, annotation)\n",
    "    return f\n",
    "\n",
    "def readFrames(dataType):\n",
    "    frames=[]\n",
    "    print(dataType)\n",
    "    ds_dir=os.path.join(dataset_dir,'{}'.format(dataType))\n",
    "    all_files = os.listdir(ds_dir)\n",
    "    all_files_image = [fn for fn in all_files if fn.startswith(image_fn) and fn.endswith(image_type)]\n",
    "    for j, fn in enumerate(all_files_image):\n",
    "        f = readImgs(ds_dir,fn)\n",
    "        frames.append(f)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames=readFrames('test')\n",
    "test_patches = DataGenerator(input_image_channel,patch_size, frames, input_label_channel, augmenter = None).all_sequential_patches(step_size)\n",
    "print('test patchs number:',len(test_patches[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Display the some of the test images\n",
    "numberOfImagesToDisplay = 5\n",
    "\n",
    "train_images, real_label = test_patches[0][:numberOfImagesToDisplay], test_patches[1][:numberOfImagesToDisplay]\n",
    "display_images(np.concatenate((train_images,real_label), axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model \n",
    "\n",
    "def evaluate_model(model_path, evaluation_report_filename):\n",
    "    print(model_path, evaluation_report_filename)\n",
    "    model = load_model(modelToEvaluate, custom_objects={'dice_loss':dice_loss, 'accuracy':accuracy , 'recall':recall, 'precision':precision,'IoU':IoU,'F1_score':F1_score}, compile=False)\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[dice_loss, accuracy,recall,precision,F1_score,IoU])\n",
    "    \n",
    "    print('Evaluating model now!')\n",
    "    ev = model.evaluate(test_patches[0], test_patches[1],  verbose=1, use_multiprocessing=False)\n",
    "    report  = dict(zip(model.metrics_names, ev))\n",
    "    report['model_path'] =  model_path   \n",
    "    report['test_frame_dir']= base_dir   \n",
    "    report['total_patch_count']= len(test_patches[0])  \n",
    "    return report\n",
    "\n",
    "report = evaluate_model(modelToEvaluate, evaluation_report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the final report\n",
    "print(report)\n",
    "\n",
    "tdf = pd.DataFrame(report, index=[0])  \n",
    "print(tdf.columns)\n",
    "col_beginning = ['model_path','test_frame_dir', 'total_patch_count', 'accuracy', 'recall','precision','IoU','F1_score']\n",
    "\n",
    "col_rest = [x for x in tdf.columns.tolist() if x not in col_beginning]\n",
    "cols = col_beginning + col_rest\n",
    "tdf = tdf[cols]\n",
    "tdf.to_csv(evaluation_report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(modelToEvaluate, custom_objects={'dice_loss':dice_loss, 'accuracy':accuracy , 'recall':recall, 'precision':precision,'IoU':IoU,'F1_score':F1_score}, compile=False)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[dice_loss, accuracy,recall,precision,F1_score,IoU])\n",
    "columns=['model_path','test_frame_dir', 'total_patch_count', 'accuracy', 'recall','precision','IoU','loss','dice_loss','F1_score']\n",
    "tdf=pd.DataFrame(columns=columns)\n",
    "\n",
    "ds_dir=os.path.join(dataset_dir,'test')\n",
    "all_files = os.listdir(ds_dir)\n",
    "# print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=pd.DataFrame(columns=columns)\n",
    "for i in range(type_num):\n",
    "    patches = []\n",
    "    frames=[]\n",
    "    files_image = [fn for fn in all_files if fn.startswith(image_fn+str(i)) and  fn.endswith(image_type)]#image.png\n",
    "    print('type{} image number:{}'.format(i,len(files_image)))\n",
    "    for j, fn in enumerate(files_image):\n",
    "        f= readImgs(ds_dir,fn)\n",
    "        frames.append(f)\n",
    "    for frame in frames:\n",
    "        ps= frame.sequential_patches(patch_size, step_size)\n",
    "        patches.extend(ps)\n",
    "    data = np.array(patches)\n",
    "    del frames,patches\n",
    "    ev = model.evaluate(data[...,:5],data[...,[5]],verbose=1, use_multiprocessing=False)\n",
    "    report  = dict(zip(model.metrics_names, ev))\n",
    "    report['model_path'] =  modelToEvaluate   \n",
    "    report['test_frame_dir']= ds_dir   \n",
    "    report['total_patch_count']= len(data)  \n",
    "\n",
    "    new_row = pd.DataFrame([report], index=[0])\n",
    "    tdf = pd.concat([tdf, new_row], ignore_index=True)\n",
    "\n",
    "evaluation_report_filename = os.path.join(evaluation_report_path,'evaluation_per_pixel{}_{}.csv'.format(timestr,chs))\n",
    "tdf.to_csv(evaluation_report_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=['ndwi','rgb','swir','annotation','prediction']\n",
    "test_images = data[i*8:i*8+8,...,:5]\n",
    "real_label = data[i*8:i*8+8,...,[5]]\n",
    "prediction = model.predict(test_images, steps=1)\n",
    "prediction[prediction>0.5]=1\n",
    "prediction[prediction<=0.5]=0\n",
    "display_images(np.concatenate((test_images, real_label, prediction), axis = -1),titles=titles)\n",
    "i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLAKESplus_env",
   "language": "python",
   "name": "glakesplus_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
